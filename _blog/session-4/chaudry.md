While reading Paul Ohm’s “Broken Promises of Privacy: Responding to the Surprising Failure of Anonymization,” I found it very interesting that the combination of zip code, birth date (including year), and sex was unique for 87% of the American population. This statistic reminds me of the “anonymous” surveys I have taken, which casually end by stating, “Please help us maintain records by entering... [some seemingly innocuous combination of information].” After reading Ohm’s article, it appears that out of the entire survey, the combination of information casually requested at the end is truly what makes the data useful. As Ohm suggests, “data can be either useful or perfectly anonymous but never both.” 

This point seems validated by the example presented by Bambauer et al. in “Fool's Gold: an Illustrated Critique of Differential Privacy” in their discussion of an internist who queries a state medical health record for patients in her city who have presented with Eastern Equine Encephalitis Virus (EEEV). In this case, the anonymization technique used, differential privacy, leads to a situation in which “the noise so badly dwarfs the true figures that the database query is a pointless ceremony.” They caution that although differential privacy is a tempting technique often employed by the legal community and policymakers, “adopting differential privacy as a regulatory best practice or mandate would be the end of research as we know it.”

Ohm’s article led me to think further about the public’s faith in anonymization. If so many industry leaders like Google state that “anonymization will make it very unlikely users could be identified,” and legal scholars hold “consensus views” that “anonymization of data is essential” and “future reidentification would be impossible,” it is not surprising that the public would trust these sentiments and have deep faith in “robust anonymization.”

For the public, I believe it is becoming crucial to understand that anonymization does not equal privacy protection. Hence, Ohm’s point becomes increasingly important when he highlights this assumption written into laws: “In addition to HIPAA and the EU Data Protection Directive, almost every single privacy statute and regulation ever written in the U.S. and the EU embraces—implicitly or explicitly, pervasively or only incidentally—the assumption that anonymization protects privacy, most often by extending safe harbors from penalty to those who anonymize their data.” However, I think Ohm falls short when he states, “At the very least, regulators must reexamine every single privacy law and regulation.” This suggestion is obviously impractical.

The eventual solution that Ohm provides, in my opinion, is something data scientists who release large datasets are already doing. Ohm states that data administrators and regulators “should weigh the benefits of unfettered information flow against the costs of privacy harms,” which I believe happens frequently when data is to be released. Ohm also suggests that risk assessment strategies that take into account reidentification should be actively considered-- something that’s not new, but should be more deeply ingrained in every data release initiative. The only unique part of Ohm’s technique to protecting information is to “regulate large entropy reducers, entities that amass massive databases containing so many links between so many disparate kinds of information that they represent a significant part of the database of ruin, even if they delete from their databases all particularly sensitive and directly linkable information.” This feature would definitely reduce the number of privacy breaches that occur due to the release of large data sets; however, I believe it will create a huge number of new problems. The criteria for “large entropy reducers” could lead to undue scrutiny, discrimination, and even competitive disadvantages. To penalize companies because they store large pieces of information could even be seen as unconstitutional. Therefore, I think Ohm’s suggestion eventually fall short. 

The only plausible technique to truly protecting people’s privacy is one that is already being attempted, albeit in different ways, which is to consider some “series of factors to identify situations in which harm is likely and whether it outweighs the benefits of unfettered information flow.” When the identified harm outweighs the benefits, the release of data should be regulated.
